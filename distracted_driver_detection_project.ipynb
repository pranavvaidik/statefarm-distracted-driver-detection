{
  "cells": [
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "a4b2842555dae4b066ecbbec8f1fa9f64f22f7de"
      },
      "cell_type": "markdown",
      "source": "# Distraction Driver Detection Project\n## Project Aim: To predict the activity of the driver from the image\nThis document is a test notebook to test and implement the codes for the Distracted Driver detection project.\n\nThis is some plain text that forms a paragraph.\nAdd emphasis via **bold** and __bold__, or *italic* and _italic_.\n\nParagraphs must be separated by an empty line.\n\n* Sometimes we want to include lists.\n * Which can be indented.\n\n1. Lists can also be numbered.\n2. For ordered lists.\n\n[It is possible to include hyperlinks](https://www.example.com)\n\nInline code uses single backticks: `foo()`, and code blocks use triple backticks:\n\n```\nbar()\n```\n\nOr can be intented by 4 spaces:\n\n    foo()\n"
    },
    {
      "metadata": {
        "_uuid": "f8f1e4263e3a7009471f82b498a8a0f450887c68"
      },
      "cell_type": "markdown",
      "source": "## Step 0: Importing the datasets\n\nWe start by importing the images from the datasets. The datasets are too large for the memory to load all the data simultaneously. So we start by loading the file names and the labels. We also split the files to train, test and validation sets here, as the actual test set peovided to us has no labels. So we are sticking to validate and test our models with the train set itself. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "72a56cadd20e473822c5234b4615001e0f79cd8c",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "from sklearn.datasets import load_files\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import np_utils\nimport numpy as np\nfrom glob import glob\n\n#defining a function to load the dataset\ndef load_dataset(path):\n    data = load_files(path)\n    driver_images = np.array(data['filenames'])\n    driver_activities = np_utils.to_categorical(np.array(data['target']))\n    return driver_images, driver_activities\n\n#loading the datasets\n#change the directory in the github as well\nimages, targets = load_dataset('../input/state-farm-distracted-driver-detection/train/')\n\n#splitting data to train, test and validation datasets\nimages_train, images_rest, targets_train, targets_rest = train_test_split( images, targets, train_size=0.8, random_state=42)\nimages_val, images_test, targets_val, targets_test = train_test_split( images_rest, targets_rest, train_size=0.5, random_state=42)\n\n\n#printing the dataset statistics\nprint('There are %d total number of driver images' % len(images))\n\n\nprint('There are %d number of train images' % len(images_train))\nprint('There are %d number of validation images' % len(images_val))\nprint('There are %d number of test images' % len(images_test))\n\n\nimport os\nprint(os.listdir(\"../input/state-farm-distracted-driver-detection/train\"))\n",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\nUsing TensorFlow backend.\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "There are 22424 total number of driver images\nThere are 17939 number of train images\nThere are 2242 number of validation images\nThere are 2243 number of test images\n['c9', 'c0', 'c8', 'c1', '.DS_Store', 'c7', 'c5', 'c6', 'c4', 'c2', 'c3']\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n  FutureWarning)\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "7cb57cfd72b17c376b50e1f2f792263a8b15b62a"
      },
      "cell_type": "markdown",
      "source": "## Step 2: Data Generator\n\nWe found our the RAM we have on this computer/kernel is not large enough to run this algorithm for this large dataset. Hence, we train the model in batches. We define a new class  inherited from the Sequence class in keras and modify it to our requirements. The batch size is set to 32 images by default, which we will increase depending on other factors. For preprocessing, we normalize the pixels in the images. Normalization and image size setting is done in the data generator class itself to reduce the need for preprocessing in  the later steps and also to reduce the size of the generated data."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b88f9b5e7553d65ba11c407f1dda5549e1dfdab9"
      },
      "cell_type": "code",
      "source": "#import cv2\n#import matplotlib.pyplot as plt\nfrom keras.preprocessing import image\nfrom tqdm import tqdm\nfrom PIL import ImageFile                            \nImageFile.LOAD_TRUNCATED_IMAGES = True\nimport keras\n#define a function that reads a path to an image and returns a tensor suitable for keras\n\n\n#Create Data Generator class that can generate data in batches and split them to train, test and validation sets\nclass DataGenerator(keras.utils.Sequence):\n    \n    #def __init__(self, list_file_paths, labels, batch_size=32, dim=(32,32,32), n_channels=1,\n    #             n_classes=10, shuffle=True):\n    def __init__(self, list_file_paths, labels, batch_size=32, shuffle=True):\n        \n        #Initialization\n        #self.dim = dim\n        self.batch_size = batch_size\n        self.labels = labels\n        self.list_file_paths = list_file_paths\n        #self.n_channels = n_channels\n        #self.n_classes = n_classes\n        self.shuffle = shuffle\n        self.on_epoch_end()\n        \n    def __len__(self):\n        #Denotes the number of batches per epoch\n        return int(np.floor(len(self.list_file_paths) / self.batch_size))\n    \n    def __getitem__(self, index):\n        #Generate one batch of data\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        list_IDs_temp = [self.list_file_paths[k] for k in indexes]\n\n        # Generate data\n        X = self.__data_generation(list_IDs_temp)\n        y = self.labels[indexes]\n\n        return X, y\n    \n    def on_epoch_end(self):\n        #Updates indexes after each epoch\n        self.indexes = np.arange(len(self.list_file_paths))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n    \n    def path_to_tensor(self, path):\n        img = image.load_img(path, target_size=(256, 256))\n        x = image.img_to_array(img)\n        return np.expand_dims(x, axis=0)\n    \n    #define a function that reads a path to an image and returns a tensor suitable for keras\n    #def paths_to_tensor(paths_list):\n    #    img_list = [path_to_tensor(img_path) for img_path in tqdm(paths_list)]\n    #    return np.vstack(img_list)\n    \n    def __data_generation(self, list_file_paths_temp):\n        \n        #Generates data containing batch_size samples\n        # X : (n_samples, *dim, n_channels)\n        # Initialization\n        img_list = [self.path_to_tensor(img_path) for img_path in list_file_paths_temp]\n        return np.vstack(img_list).astype('float32')/255\n\n\n\n               \n\n#normalize all images: divide the tensors by 255\n#pre-process the data for Keras\n#train_tensors = paths_to_tensor(images_train)#.astype('float32')/255\n#valid_tensors = paths_to_tensor(images_val)#.astype('float32')/255\n#test_tensors = paths_to_tensor(images_test)#.astype('float32')/255\n\nprint(\"atleast this is working!\")\n\n",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "atleast this is working!\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "67620692c37e15ab9c8c652f01c0934c7a40b8af",
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# Step 3 : Instantiate data generator objects\n\nHere, we create the data generator objects for train. test and validation objects. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "60ca1d88d18455213ec22b0061487fd1b9feb2fb",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Parameters for the Data Generator\nparams = {'batch_size' : 64,\n          'shuffle': True}\n\n# creating generators\ntraining_generator = DataGenerator(images_train, targets_train, **params)\nvalidation_generator = DataGenerator(images_val, targets_val, **params)\ntesting_generator = DataGenerator(images_test, targets_test, **params)\n\n\n\n\n",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "13ede2a266b741c0a22b9e446fc89e72c8c020b4",
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# Step 4 : Construct & Train the Vanilla model\n\nHere, we construct a Vanilla CNN model. It should contain a basic CNN layer, followed by RELU, Maxpool and Softmax layers respectively in that order. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c0d84d7be4b3cf450b8bc94623088870b84c90a0"
      },
      "cell_type": "code",
      "source": "\nfrom  keras.layers  import Conv2D, MaxPooling2D, GlobalAveragePooling2D\nfrom keras.layers import Dropout, Flatten, Dense\nfrom keras.models import Sequential\n\nmodel = Sequential()\n\n### TODO: Define your architecture.\n\nmodel.add(Conv2D(filters = 32, kernel_size = 3, strides = 1, padding = 'same', activation = None, input_shape = (256,256,3)))\n#model.add(MaxPooling2D(pool_size=(2, 2), strides=2, padding='same'))\n#model.add(Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', activation = None))\n#model.add(MaxPooling2D(pool_size=(2, 2), strides=2, padding='same'))\n#model.add(Conv2D(filters = 128, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu'))\n#model.add(MaxPooling2D(pool_size=(2, 2), strides=2, padding='same'))\n#model.add(Conv2D(filters = 256, kernel_size = 3, strides = 2, padding = 'same', activation = 'relu'))\nmodel.add(GlobalAveragePooling2D())\n\n#model.add(Dropout(0.5))\n\nmodel.add(Dense(10,activation = 'softmax'))\n\n         \n\nmodel.summary()\n\n\n\n\n",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_1 (Conv2D)            (None, 256, 256, 32)      896       \n_________________________________________________________________\nglobal_average_pooling2d_1 ( (None, 32)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 10)                330       \n=================================================================\nTotal params: 1,226\nTrainable params: 1,226\nNon-trainable params: 0\n_________________________________________________________________\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "06bb994cdfa1329bcc069a1bce30abfba2704f04"
      },
      "cell_type": "markdown",
      "source": "Compiling the model"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "2ff779d35aa014255d31bbb68dbc741d9faca9c5"
      },
      "cell_type": "code",
      "source": "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0b70054c6fa32557c6dd40ccbb2c44b2c7344614"
      },
      "cell_type": "markdown",
      "source": "We train the vanilla model here"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b674e1f960b13d25b9931b22e291bbfb6b301a06"
      },
      "cell_type": "code",
      "source": "from keras.callbacks import ModelCheckpoint  \n\n### TODO: specify the number of epochs that you would like to use to train the model.\n\nepochs = 25\n\n### .\n\ncheckpointer = ModelCheckpoint(filepath='weights.best.from_scratch.hdf5', \n                               verbose=1, save_best_only=True)\n\nmodel.fit_generator(generator=training_generator, validation_data=validation_generator, epochs= epochs, callbacks=[checkpointer], use_multiprocessing=True, workers=6)\n",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Epoch 1/25\n280/280 [==============================] - 174s 621ms/step - loss: 2.2995 - acc: 0.1164 - val_loss: 2.3014 - val_acc: 0.1013\n\nEpoch 00001: val_loss improved from inf to 2.30143, saving model to weights.best.from_scratch.hdf5\nEpoch 2/25\n280/280 [==============================] - 171s 611ms/step - loss: 2.2986 - acc: 0.1152 - val_loss: 2.2972 - val_acc: 0.1187\n\nEpoch 00002: val_loss improved from 2.30143 to 2.29722, saving model to weights.best.from_scratch.hdf5\nEpoch 3/25\n280/280 [==============================] - 171s 612ms/step - loss: 2.2978 - acc: 0.1148 - val_loss: 2.2971 - val_acc: 0.1152\n\nEpoch 00003: val_loss improved from 2.29722 to 2.29712, saving model to weights.best.from_scratch.hdf5\nEpoch 4/25\n280/280 [==============================] - 172s 613ms/step - loss: 2.2957 - acc: 0.1179 - val_loss: 2.2975 - val_acc: 0.1134\n\nEpoch 00004: val_loss did not improve\nEpoch 5/25\n280/280 [==============================] - 177s 633ms/step - loss: 2.2947 - acc: 0.1175 - val_loss: 2.2955 - val_acc: 0.1317\n\nEpoch 00005: val_loss improved from 2.29712 to 2.29552, saving model to weights.best.from_scratch.hdf5\nEpoch 6/25\n280/280 [==============================] - 177s 632ms/step - loss: 2.2929 - acc: 0.1209 - val_loss: 2.2959 - val_acc: 0.1353\n\nEpoch 00006: val_loss did not improve\nEpoch 7/25\n280/280 [==============================] - 178s 636ms/step - loss: 2.2916 - acc: 0.1224 - val_loss: 2.2948 - val_acc: 0.1183\n\nEpoch 00007: val_loss improved from 2.29552 to 2.29476, saving model to weights.best.from_scratch.hdf5\nEpoch 8/25\n280/280 [==============================] - 178s 635ms/step - loss: 2.2905 - acc: 0.1249 - val_loss: 2.2935 - val_acc: 0.1241\n\nEpoch 00008: val_loss improved from 2.29476 to 2.29355, saving model to weights.best.from_scratch.hdf5\nEpoch 9/25\n280/280 [==============================] - 178s 637ms/step - loss: 2.2893 - acc: 0.1269 - val_loss: 2.2954 - val_acc: 0.1161\n\nEpoch 00009: val_loss did not improve\nEpoch 10/25\n280/280 [==============================] - 177s 632ms/step - loss: 2.2880 - acc: 0.1262 - val_loss: 2.2946 - val_acc: 0.1335\n\nEpoch 00010: val_loss did not improve\nEpoch 11/25\n280/280 [==============================] - 170s 608ms/step - loss: 2.2870 - acc: 0.1307 - val_loss: 2.2922 - val_acc: 0.1214\n\nEpoch 00011: val_loss improved from 2.29355 to 2.29218, saving model to weights.best.from_scratch.hdf5\nEpoch 12/25\n280/280 [==============================] - 170s 607ms/step - loss: 2.2867 - acc: 0.1298 - val_loss: 2.2942 - val_acc: 0.1174\n\nEpoch 00012: val_loss did not improve\nEpoch 13/25\n280/280 [==============================] - 171s 609ms/step - loss: 2.2859 - acc: 0.1320 - val_loss: 2.2913 - val_acc: 0.1263\n\nEpoch 00013: val_loss improved from 2.29218 to 2.29129, saving model to weights.best.from_scratch.hdf5\nEpoch 14/25\n280/280 [==============================] - 175s 624ms/step - loss: 2.2860 - acc: 0.1330 - val_loss: 2.2915 - val_acc: 0.1357\n\nEpoch 00014: val_loss did not improve\nEpoch 15/25\n280/280 [==============================] - 178s 635ms/step - loss: 2.2856 - acc: 0.1360 - val_loss: 2.2943 - val_acc: 0.1272\n\nEpoch 00015: val_loss did not improve\nEpoch 16/25\n280/280 [==============================] - 177s 632ms/step - loss: 2.2846 - acc: 0.1297 - val_loss: 2.2903 - val_acc: 0.1187\n\nEpoch 00016: val_loss improved from 2.29129 to 2.29030, saving model to weights.best.from_scratch.hdf5\nEpoch 17/25\n280/280 [==============================] - 177s 633ms/step - loss: 2.2845 - acc: 0.1323 - val_loss: 2.2941 - val_acc: 0.1241\n\nEpoch 00017: val_loss did not improve\nEpoch 18/25\n280/280 [==============================] - 178s 635ms/step - loss: 2.2829 - acc: 0.1309 - val_loss: 2.2961 - val_acc: 0.1254\n\nEpoch 00018: val_loss did not improve\nEpoch 19/25\n280/280 [==============================] - 178s 637ms/step - loss: 2.2826 - acc: 0.1344 - val_loss: 2.2966 - val_acc: 0.1241\n\nEpoch 00019: val_loss did not improve\nEpoch 20/25\n280/280 [==============================] - 178s 634ms/step - loss: 2.2827 - acc: 0.1375 - val_loss: 2.2988 - val_acc: 0.1304\n\nEpoch 00020: val_loss did not improve\nEpoch 21/25\n280/280 [==============================] - 178s 637ms/step - loss: 2.2830 - acc: 0.1354 - val_loss: 2.2890 - val_acc: 0.1321\n\nEpoch 00021: val_loss improved from 2.29030 to 2.28901, saving model to weights.best.from_scratch.hdf5\nEpoch 22/25\n280/280 [==============================] - 178s 634ms/step - loss: 2.2821 - acc: 0.1317 - val_loss: 2.2920 - val_acc: 0.1246\n\nEpoch 00022: val_loss did not improve\nEpoch 23/25\n280/280 [==============================] - 179s 639ms/step - loss: 2.2819 - acc: 0.1365 - val_loss: 2.2921 - val_acc: 0.1232\n\nEpoch 00023: val_loss did not improve\nEpoch 24/25\n280/280 [==============================] - 178s 635ms/step - loss: 2.2821 - acc: 0.1366 - val_loss: 2.2894 - val_acc: 0.1321\n\nEpoch 00024: val_loss did not improve\nEpoch 25/25\n280/280 [==============================] - 177s 634ms/step - loss: 2.2813 - acc: 0.1357 - val_loss: 2.2889 - val_acc: 0.1268\n\nEpoch 00025: val_loss improved from 2.28901 to 2.28895, saving model to weights.best.from_scratch.hdf5\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "<keras.callbacks.History at 0x7f22483c7ac8>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "7e8cfba31a1692abbda40fef466c690fe52298f6"
      },
      "cell_type": "markdown",
      "source": "# Step 5 : Test the Vanilla Model\n\nHere we test the vanilla model and get the accuracy of the model. We also calculate the multiclass logloss value in this step\n"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "0ab58871ebd27398a0cbe8be89877e7f8ef28311"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3c434c7bf6c18124d276e63f9794ad42416f38ed"
      },
      "cell_type": "markdown",
      "source": "# Step 6 : Load RESNET model"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "c864e0c525e8b783cc18ce27835806323b9efb86"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "acea1e5b7f6cd2491798316edaceda526127df54"
      },
      "cell_type": "markdown",
      "source": "# Step 7 : Transfer Learning using the RESNET model"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "70b6d047ffbe6a0f9a9c543620b8d92d7a9c9755"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4c26218279a2f5a30ac02d66e3c708ff74a281af"
      },
      "cell_type": "markdown",
      "source": "# Step 8 : Test the new model"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "f6a9b806fede56e073c95cfe67d383fc28801900"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7b036d0d9abd96c62e5bc26fc46438fac5a8e254"
      },
      "cell_type": "markdown",
      "source": "# Step 9 : Show some image results with the new model"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "058fff3bb840b364e8486d89a4f19a5ef51af750"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e59218b2fafdfbdd2771d6109bc17ab9f0c6391e"
      },
      "cell_type": "markdown",
      "source": "# Step 10 : Conclusion"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "123859daa7179394c462e6e1869bce2586aaff68"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}