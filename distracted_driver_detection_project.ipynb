{
  "cells": [
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "a4b2842555dae4b066ecbbec8f1fa9f64f22f7de"
      },
      "cell_type": "markdown",
      "source": "# Distraction Driver Detection Project\n## Project Aim: To predict the activity of the driver from the image\nThis document is a test notebook to test and implement the codes for the Distracted Driver detection project.\n\nThis is some plain text that forms a paragraph.\nAdd emphasis via **bold** and __bold__, or *italic* and _italic_.\n\nParagraphs must be separated by an empty line.\n\n* Sometimes we want to include lists.\n * Which can be indented.\n\n1. Lists can also be numbered.\n2. For ordered lists.\n\n[It is possible to include hyperlinks](https://www.example.com)\n\nInline code uses single backticks: `foo()`, and code blocks use triple backticks:\n\n```\nbar()\n```\n\nOr can be intented by 4 spaces:\n\n    foo()\n"
    },
    {
      "metadata": {
        "_uuid": "f8f1e4263e3a7009471f82b498a8a0f450887c68"
      },
      "cell_type": "markdown",
      "source": "## Step 0: Importing the datasets\n\nWe start by importing the images from the datasets. The datasets are too large for the memory to load all the data simultaneously. So we start by loading the file names and the labels. We also split the files to train, test and validation sets here, as the actual test set peovided to us has no labels. So we are sticking to validate and test our models with the train set itself. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "72a56cadd20e473822c5234b4615001e0f79cd8c",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "from sklearn.datasets import load_files\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import np_utils\nimport numpy as np\nfrom glob import glob\n\n#defining a function to load the dataset\ndef load_dataset(path):\n    data = load_files(path)\n    driver_images = np.array(data['filenames'])\n    driver_activities = np_utils.to_categorical(np.array(data['target']))\n    return driver_images, driver_activities\n\n#loading the datasets\n#change the directory in the github as well\nimages, targets = load_dataset('../input/state-farm-distracted-driver-detection/train/')\n\n#splitting data to train, test and validation datasets\nimages_train, images_rest, targets_train, targets_rest = train_test_split( images, targets, train_size=0.8, random_state=42)\nimages_val, images_test, targets_val, targets_test = train_test_split( images_rest, targets_rest, train_size=0.5, random_state=42)\n\n\n#printing the dataset statistics\nprint('There are %d total number of driver images' % len(images))\n\n\nprint('There are %d number of train images' % len(images_train))\nprint('There are %d number of validation images' % len(images_val))\nprint('There are %d number of test images' % len(images_test))\n\n\nimport os\nprint(os.listdir(\"../input/state-farm-distracted-driver-detection/train\"))\n",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\nUsing TensorFlow backend.\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "There are 22424 total number of driver images\nThere are 17939 number of train images\nThere are 2242 number of validation images\nThere are 2243 number of test images\n['c9', 'c0', 'c8', 'c1', '.DS_Store', 'c7', 'c5', 'c6', 'c4', 'c2', 'c3']\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n  FutureWarning)\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "7cb57cfd72b17c376b50e1f2f792263a8b15b62a"
      },
      "cell_type": "markdown",
      "source": "## Step 2: Data Generator\n\nWe found our the RAM we have on this computer/kernel is not large enough to run this algorithm for this large dataset. Hence, we train the model in batches. We define a new class  inherited from the Sequence class in keras and modify it to our requirements. The batch size is set to 32 images by default, which we will increase depending on other factors. For preprocessing, we normalize the pixels in the images. Normalization and image size setting is done in the data generator class itself to reduce the need for preprocessing in  the later steps and also to reduce the size of the generated data."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b88f9b5e7553d65ba11c407f1dda5549e1dfdab9"
      },
      "cell_type": "code",
      "source": "#import cv2\n#import matplotlib.pyplot as plt\nfrom keras.preprocessing import image\nfrom tqdm import tqdm\nfrom PIL import ImageFile                            \nImageFile.LOAD_TRUNCATED_IMAGES = True\nimport keras\n#define a function that reads a path to an image and returns a tensor suitable for keras\n\n\n#Create Data Generator class that can generate data in batches and split them to train, test and validation sets\nclass DataGenerator(keras.utils.Sequence):\n    \n    #def __init__(self, list_file_paths, labels, batch_size=32, dim=(32,32,32), n_channels=1,\n    #             n_classes=10, shuffle=True):\n    def __init__(self, list_file_paths, labels, batch_size=32, shuffle=True):\n        \n        #Initialization\n        #self.dim = dim\n        self.batch_size = batch_size\n        self.labels = labels\n        self.list_file_paths = list_file_paths\n        #self.n_channels = n_channels\n        #self.n_classes = n_classes\n        self.shuffle = shuffle\n        self.on_epoch_end()\n        \n    def __len__(self):\n        #Denotes the number of batches per epoch\n        return int(np.floor(len(self.list_file_paths) / self.batch_size))\n    \n    def __getitem__(self, index):\n        #Generate one batch of data\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n\n        # Generate data\n        X = self.__data_generation(list_IDs_temp)\n        y = self.labels[indexes]\n\n        return X, y\n    \n    def on_epoch_end(self):\n        #Updates indexes after each epoch\n        self.indexes = np.arange(len(self.list_file_paths))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n    \n    def path_to_tensor(path):\n        img = image.load_img(path, target_size=(256, 256))\n        x = image.img_to_array(img)\n        return np.expand_dims(x, axis=0)\n    \n    #define a function that reads a path to an image and returns a tensor suitable for keras\n    #def paths_to_tensor(paths_list):\n    #    img_list = [path_to_tensor(img_path) for img_path in tqdm(paths_list)]\n    #    return np.vstack(img_list)\n    \n    def __data_generation(self, list_file_paths_temp):\n        \n        #Generates data containing batch_size samples\n        # X : (n_samples, *dim, n_channels)\n        # Initialization\n        img_list = [path_to_tensor(img_path) for img_path in list_file_paths_temp]\n        return np.vstack(img_list).astype('float32')/255\n\n\n\n               \n\n#normalize all images: divide the tensors by 255\n#pre-process the data for Keras\n#train_tensors = paths_to_tensor(images_train)#.astype('float32')/255\n#valid_tensors = paths_to_tensor(images_val)#.astype('float32')/255\n#test_tensors = paths_to_tensor(images_test)#.astype('float32')/255\n\nprint(\"atleast this is working!\")\n\n",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "atleast this is working!\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "67620692c37e15ab9c8c652f01c0934c7a40b8af",
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# Step 3 : Instantiate data generator objects\n\nHere, we create the data generator objects for train. test and validation objects. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "60ca1d88d18455213ec22b0061487fd1b9feb2fb"
      },
      "cell_type": "code",
      "source": "from keras.models import Sequential\n\n# Parameters for the Data Generator\nparams = {'batch_size' : 64,\n          'shuffle': True}\n\n# creating generators\ntraining_generator = DataGenerator(images_train, targets_train, **params)\nvalidation_generator = DataGenerator(images_val, targets_val, **params)\ntesting_generator = DataGenerator(images_test, targets_test, **params)\n\n\n",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "13ede2a266b741c0a22b9e446fc89e72c8c020b4",
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# Step 4 : Construct & Train the Vanilla model\n\nHere, we construct a Vanilla CNN model. It should contain a basic CNN layer, followed by RELU, Maxpool and Softmax layers respectively in that order. "
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "c0d84d7be4b3cf450b8bc94623088870b84c90a0"
      },
      "cell_type": "code",
      "source": "test_tensors = paths_to_tensor(images_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7e8cfba31a1692abbda40fef466c690fe52298f6"
      },
      "cell_type": "markdown",
      "source": "# Step 5 : Test the Vanilla Model\n\nHere we test the vanilla model and get the accuracy of the model. We also calculate the multiclass logloss value in this step\n"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "0ab58871ebd27398a0cbe8be89877e7f8ef28311"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3c434c7bf6c18124d276e63f9794ad42416f38ed"
      },
      "cell_type": "markdown",
      "source": "# Step 6 : Load RESNET model"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "c864e0c525e8b783cc18ce27835806323b9efb86"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "acea1e5b7f6cd2491798316edaceda526127df54"
      },
      "cell_type": "markdown",
      "source": "# Step 7 : Transfer Learning using the RESNET model"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "70b6d047ffbe6a0f9a9c543620b8d92d7a9c9755"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4c26218279a2f5a30ac02d66e3c708ff74a281af"
      },
      "cell_type": "markdown",
      "source": "# Step 8 : Test the new model"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "f6a9b806fede56e073c95cfe67d383fc28801900"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7b036d0d9abd96c62e5bc26fc46438fac5a8e254"
      },
      "cell_type": "markdown",
      "source": "# Step 9 : Show some image results with the new model"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "058fff3bb840b364e8486d89a4f19a5ef51af750"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e59218b2fafdfbdd2771d6109bc17ab9f0c6391e"
      },
      "cell_type": "markdown",
      "source": "# Step 10 : Conclusion"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "123859daa7179394c462e6e1869bce2586aaff68"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}